{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of the Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.linalg import solve\n",
    "from scipy.interpolate import BSpline\n",
    "from scipy.special import erf\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "def run_simulation(g):\n",
    "    n = 500  # Define the number of samples\n",
    "\n",
    "    # Simulate data from uniform distributions\n",
    "    x1 = np.random.uniform(-1, 1, n)\n",
    "    x2 = np.random.uniform(0, 1, n)\n",
    "    x3 = np.random.uniform(-1, 1, n)\n",
    "    x4 = np.random.uniform(0, 1, n)\n",
    "    x5 = np.random.uniform(-1, 1, n)\n",
    "    x6 = np.random.uniform(-1, 1, n)\n",
    "    x_uniform = np.random.uniform(-1, 1, (n, 14))  # Uniform data with 14 features\n",
    "\n",
    "    # Define several functions f1 to f6\n",
    "    def f1_def(x):\n",
    "        return  2*(np.cos( 2* np.pi * np.sqrt((x - 0.5) ** 2))).flatten()\n",
    "\n",
    "    def f2_def(x: np.ndarray) -> np.ndarray:\n",
    "        return 9 / (1 + np.exp(-10 * x))  # Logistic function\n",
    "\n",
    "    def f3_def(x: np.ndarray) -> np.ndarray:\n",
    "        return 3 * x**3  # Cubic transformation\n",
    "\n",
    "    def f4_def(x: np.ndarray) -> np.ndarray:\n",
    "        return x ** 3 + 1.5 * x ** 2  # Cubic plus quadratic\n",
    "\n",
    "    def f5_def(x: np.ndarray) -> np.ndarray:\n",
    "        return np.cos(6 * x - 6) - 1.25 * x  # Combination of cosine and linear terms\n",
    "\n",
    "    def f6_def(x: np.ndarray) -> np.ndarray:\n",
    "        return 1.6 * x  # Linear transformation\n",
    "\n",
    "    # Center the functions by subtracting their mean\n",
    "    f1 = f1_def(x1) - np.mean(f1_def(x1))\n",
    "    f2 = f2_def(x2) - np.mean(f2_def(x2))\n",
    "    f3 = f3_def(x3) - np.mean(f3_def(x3))\n",
    "    f4 = f4_def(x4) - np.mean(f4_def(x4))\n",
    "    f5 = f5_def(x5) - np.mean(f5_def(x5))\n",
    "    f6 = f6_def(x6) - np.mean(f6_def(x6))\n",
    "\n",
    "    # Construct the additive model\n",
    "    f_sum = f1 + f2 + f3 + f4 + f5 + f6   \n",
    "    variance = g * np.var(f_sum)  \n",
    "    y = f_sum + np.random.normal(0, np.sqrt(variance), n)  # Add noise to the signal\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_size = 400\n",
    "    test_size = n - train_size\n",
    "\n",
    "    indices = np.arange(n)\n",
    "    train_indices, test_indices = train_test_split(indices, train_size=train_size, shuffle=False, random_state=0)\n",
    "\n",
    "    x1_train, x1_test = x1[train_indices], x1[test_indices]\n",
    "    x2_train, x2_test = x2[train_indices], x2[test_indices]\n",
    "    x3_train, x3_test = x3[train_indices], x3[test_indices]\n",
    "    x4_train, x4_test = x4[train_indices], x4[test_indices]\n",
    "    x5_train, x5_test = x5[train_indices], x5[test_indices]\n",
    "    x6_train, x6_test = x6[train_indices], x6[test_indices]\n",
    "    x_uniform_train, x_uniform_test = x_uniform[train_indices], x_uniform[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    # Generate extended range for theorical curve looks smoother \n",
    "    x1_train_ext = np.linspace(np.min(x1_train), np.max(x1_train), train_size)\n",
    "    x2_train_ext = np.linspace(np.min(x2_train), np.max(x2_train), train_size)\n",
    "    x3_train_ext = np.linspace(np.min(x3_train), np.max(x3_train), train_size)\n",
    "    x4_train_ext = np.linspace(np.min(x4_train), np.max(x4_train), train_size)\n",
    "    x5_train_ext = np.linspace(np.min(x5_train), np.max(x5_train), train_size)\n",
    "    x6_train_ext = np.linspace(np.min(x6_train), np.max(x6_train), train_size)\n",
    "\n",
    "    # Compute training functions based on the training data\n",
    "    f1_train = f1_def(x1_train) - np.mean(f1_def(x1_train))\n",
    "    f2_train = f2_def(x2_train) - np.mean(f2_def(x2_train))\n",
    "    f3_train = f3_def(x3_train) - np.mean(f3_def(x3_train))\n",
    "    f4_train = f4_def(x4_train) - np.mean(f4_def(x4_train))\n",
    "    f5_train = f5_def(x5_train) - np.mean(f5_def(x5_train))\n",
    "    f6_train = f6_def(x6_train) - np.mean(f6_def(x6_train))\n",
    "\n",
    "    # Compute training extended functions for extended ranges\n",
    "    f1_train_ext = f1_def(x1_train_ext) - np.mean(f1_def(x1_train_ext))\n",
    "    f2_train_ext = f2_def(x2_train_ext) - np.mean(f2_def(x2_train_ext))\n",
    "    f3_train_ext = f3_def(x3_train_ext) - np.mean(f3_def(x3_train_ext))\n",
    "    f4_train_ext = f4_def(x4_train_ext) - np.mean(f4_def(x4_train_ext))\n",
    "    f5_train_ext = f5_def(x5_train_ext) - np.mean(f5_def(x5_train_ext))\n",
    "    f6_train_ext = f6_def(x6_train_ext) - np.mean(f6_def(x6_train_ext))\n",
    "\n",
    "    K = 6  # Number of selected covariates\n",
    "    num_nodes = 20  # Number of nodes for B-splines\n",
    "    lambdas = np.logspace(-4, 4, 100)  # Lambda values for regularization\n",
    "\n",
    "    # Define constraints for optimization depending on the Scenario\n",
    "    constraints = []    # Scenario 1: No constraints\n",
    "    #constraints = ['f2_concavity', 'f3_monotonicity', 'f4_convexity']    # Scenario 2\n",
    "    #constraints = ['f2_concavity', 'f3_monotonicity', 'f4_convexity', 'x8_monotonicity_decrec', 'x15_concavity','x19_convexity']  # Scenario 3\n",
    "    #constraints = ['x8_monotonicity_decrec', 'x15_concavity','x19_convexity']   # Scenario 4\n",
    "\n",
    "    # Function to find the optimal lambda using generalized cross-validation (GCV)\n",
    "    def optimal_lambda_gcv(B, y, lambdas):\n",
    "        D = np.diff(np.diag(np.ones(B.shape[1])), n=2).T  \n",
    "        n, m = B.shape\n",
    "        gcv_values = []\n",
    "        for lambd in lambdas:\n",
    "            P = lambd * np.dot(np.transpose(D), D)  \n",
    "            H = B @ solve(B.T @ B + P, B.T) \n",
    "            y_hat = H @ y  # Predicted values\n",
    "            residuals = y - y_hat  # Residuals\n",
    "            h = np.diag(H)  # Diagonal elements of H\n",
    "            gcv = np.sum((residuals ** 2) / ((1 - h) ** 2)) / n  # GCV criterion\n",
    "            gcv_values.append(gcv)\n",
    "\n",
    "        gcv_values = np.array(gcv_values)\n",
    "        optimal_lambda = lambdas[np.argmin(gcv_values)]  # Select lambda that minimizes GCV\n",
    "\n",
    "        return optimal_lambda, gcv_values\n",
    "\n",
    "    # Optimization function \n",
    "    def optimization_function(x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x_uniform_train,\n",
    "                              x1_train_ext, x2_train_ext, x3_train_ext, x4_train_ext, x5_train_ext, x6_train_ext,\n",
    "                              x1_test, x2_test, x3_test, x4_test, x5_test, x6_test, x_uniform_test, \n",
    "                              y_train, y_test, K, num_nodes, lambdas, constraints):\n",
    "\n",
    "        # B-spline basis function generator\n",
    "        def my_bbase(x, xl, xr, ndx=10, bdeg=3, eps=1e-05):\n",
    "            dx = (xr - xl) / ndx\n",
    "            knots = np.linspace(xl - bdeg * dx, xr + bdeg * dx, ndx + 2 * bdeg)  # Knot sequence\n",
    "            c = np.eye(len(knots) - bdeg) \n",
    "            B = BSpline(knots, c, bdeg)(x)  # Evaluate the spline basis at points x\n",
    "            return B\n",
    "\n",
    "        # Create B-spline basis for each covariable\n",
    "        B1 = my_bbase(x1_train, np.min(x1_train), np.max(x1_train), num_nodes, 3).squeeze()\n",
    "        B2 = my_bbase(x2_train, np.min(x2_train), np.max(x2_train), num_nodes, 3).squeeze()\n",
    "        B3 = my_bbase(x3_train, np.min(x3_train), np.max(x3_train), num_nodes, 3).squeeze()\n",
    "        B4 = my_bbase(x4_train, np.min(x4_train), np.max(x4_train), num_nodes, 3).squeeze()\n",
    "        B5 = my_bbase(x5_train, np.min(x5_train), np.max(x5_train), num_nodes, 3).squeeze()\n",
    "        B6 = my_bbase(x6_train, np.min(x6_train), np.max(x6_train), num_nodes, 3).squeeze()\n",
    "        B_uniform = np.hstack([my_bbase(x_uniform_train[:, i], -1, 1 , num_nodes, 3).squeeze() for i in range(14)])\n",
    "       \n",
    "        B1_ext = my_bbase(x1_train_ext, np.min(x1_train_ext), np.max(x1_train_ext), num_nodes, 3).squeeze()\n",
    "        B2_ext = my_bbase(x2_train_ext, np.min(x2_train_ext), np.max(x2_train_ext), num_nodes, 3).squeeze()\n",
    "        B3_ext = my_bbase(x3_train_ext, np.min(x3_train_ext), np.max(x3_train_ext), num_nodes, 3).squeeze()\n",
    "        B4_ext = my_bbase(x4_train_ext, np.min(x4_train_ext), np.max(x4_train_ext), num_nodes, 3).squeeze()\n",
    "        B5_ext = my_bbase(x5_train_ext, np.min(x5_train_ext), np.max(x5_train_ext), num_nodes, 3).squeeze()\n",
    "        B6_ext = my_bbase(x6_train_ext, np.min(x6_train_ext), np.max(x6_train_ext), num_nodes, 3).squeeze()\n",
    "\n",
    "        B1_test = my_bbase(x1_test, np.min(x1_train), np.max(x1_train), num_nodes, 3).squeeze()\n",
    "        B2_test = my_bbase(x2_test, np.min(x2_train), np.max(x2_train), num_nodes, 3).squeeze()\n",
    "        B3_test = my_bbase(x3_test, np.min(x3_train), np.max(x3_train), num_nodes, 3).squeeze()\n",
    "        B4_test = my_bbase(x4_test, np.min(x4_train), np.max(x4_train), num_nodes, 3).squeeze()\n",
    "        B5_test = my_bbase(x5_test, np.min(x5_train), np.max(x5_train), num_nodes, 3).squeeze()\n",
    "        B6_test = my_bbase(x6_test, np.min(x6_train), np.max(x6_train), num_nodes, 3).squeeze()\n",
    "        B_uniform_test = np.hstack([my_bbase(x_uniform_test[:, i], -1, 1, num_nodes, 3).squeeze() for i in range(14)])\n",
    "\n",
    "        # Select the optimal lambda for each of the B matrices using GCV\n",
    "        lambda1, _ = optimal_lambda_gcv(B1, y_train, lambdas)\n",
    "        lambda2, _ = optimal_lambda_gcv(B2, y_train, lambdas)\n",
    "        lambda3, _ = optimal_lambda_gcv(B3, y_train, lambdas)\n",
    "        lambda4, _ = optimal_lambda_gcv(B4, y_train, lambdas)\n",
    "        lambda5, _ = optimal_lambda_gcv(B5, y_train, lambdas)\n",
    "        lambda6, _ = optimal_lambda_gcv(B6, y_train, lambdas)\n",
    "        lambda_uniform_list = [optimal_lambda_gcv(B_uniform[:, i * 23:(i + 1) * 23], y_train, lambdas)[0] for i in range(14)]\n",
    "\n",
    "        # Calculate penalty matrices for each basis\n",
    "        D1 = np.diff(np.diag(np.ones(B1.shape[1])), n=2).T\n",
    "        P1 = lambda1 * np.dot(np.transpose(D1), D1)\n",
    "        D2 = np.diff(np.diag(np.ones(B2.shape[1])), n=2).T\n",
    "        P2 = lambda2 * np.dot(np.transpose(D2), D2)\n",
    "        D3 = np.diff(np.diag(np.ones(B3.shape[1])), n=2).T\n",
    "        P3 = lambda3 * np.dot(np.transpose(D3), D3)\n",
    "        D4 = np.diff(np.diag(np.ones(B4.shape[1])), n=2).T\n",
    "        P4 = lambda4 * np.dot(np.transpose(D4), D4)\n",
    "        D5 = np.diff(np.diag(np.ones(B5.shape[1])), n=2).T\n",
    "        P5 = lambda5 * np.dot(np.transpose(D5), D5)\n",
    "        D6 = np.diff(np.diag(np.ones(B6.shape[1])), n=2).T\n",
    "        P6 = lambda6 * np.dot(np.transpose(D6), D6)\n",
    "\n",
    "        # Create a column vector of ones\n",
    "        X = np.ones((len(x1_train), 1))\n",
    "\n",
    "        # For identifiability issue\n",
    "        PP1 = P1 + np.dot(np.dot(np.dot(B1.T, X), X.T), B1)\n",
    "        PP2 = P2 + np.dot(np.dot(np.dot(B2.T, X), X.T), B2)\n",
    "        PP3 = P3 + np.dot(np.dot(np.dot(B3.T, X), X.T), B3)\n",
    "        PP4 = P4 + np.dot(np.dot(np.dot(B4.T, X), X.T), B4)\n",
    "        PP5 = P5 + np.dot(np.dot(np.dot(B5.T, X), X.T), B5)\n",
    "        PP6 = P6 + np.dot(np.dot(np.dot(B6.T, X), X.T), B6)\n",
    "\n",
    "        PP_uniform_list = []\n",
    "        for i in range(14):\n",
    "            start_col = i * 23\n",
    "            end_col = start_col + 23\n",
    "            B_uni = B_uniform[:, start_col:end_col]\n",
    "            D_uniform = np.diff(np.diag(np.ones(B_uni.shape[1])), n=2).T\n",
    "            P_uniform = lambda_uniform_list[i] * np.dot(np.transpose(D_uniform), D_uniform)\n",
    "            PP_uni = P_uniform + np.dot(np.dot(np.dot(B_uni.T, X), X.T), B_uni)\n",
    "            PP_uniform_list.append(PP_uni)\n",
    "\n",
    "        # Dimensions of penalty matrices\n",
    "        n1, m1 = PP1.shape\n",
    "        n2, m2 = PP2.shape\n",
    "        n3, m3 = PP3.shape\n",
    "        n4, m4 = PP4.shape\n",
    "        n5, m5 = PP5.shape\n",
    "        n6, m6 = PP6.shape\n",
    "        n_uniform = [P.shape[0] for P in PP_uniform_list]\n",
    "        m_uniform = [P.shape[1] for P in PP_uniform_list]\n",
    "        \n",
    "        total_n = 1 + n1 + n2 + n3 + n4 + n5 + n6 + sum(n_uniform)\n",
    "        total_m = 1 + m1 + m2 + m3 + m4 + m5 + m6 + sum(m_uniform)\n",
    "\n",
    "        # Create the complete penalty matrix PP\n",
    "        PP = np.zeros((total_n, total_m))\n",
    "\n",
    "        PP[1:1 + n1, 1:1 + m1] = PP1\n",
    "        PP[1 + n1:1 + n1 + n2, 1 + m1:1 + m1 + m2] = PP2\n",
    "        PP[1 + n1 + n2:1 + n1 + n2 + n3, 1 + m1 + m2:1 + m1 + m2 + m3] = PP3\n",
    "        PP[1 + n1 + n2 + n3:1 + n1 + n2 + n3 + n4, 1 + m1 + m2 + m3 + m4:1 + m1 + m2 + m3 + m4 + m5] = PP4\n",
    "        PP[1 + n1 + n2 + n3 + n4:1 + n1 + n2 + n3 + n4 + n5, 1 + m1 + m2 + m3 + m4 + m5:1 + m1 + m2 + m3 + m4 + m5 + m6] = PP5\n",
    "        PP[1 + n1 + n2 + n3 + n4 + n5:1 + n1 + n2 + n3 + n4 + n5 + n6, 1 + m1 + m2 + m3 + m4 + m5 + m6:1 + m1 + m2 + m3 + m4 + m5 + m6 + m6] = PP6\n",
    "\n",
    "        start_idx_n = 1 + n1 + n2 + n3 + n4 + n5 + n6\n",
    "        start_idx_m = 1 + m1 + m2 + m3 + m4 + m5 + m6\n",
    "        for i, PP_uniform in enumerate(PP_uniform_list):\n",
    "            n, m = PP_uniform.shape\n",
    "            PP[start_idx_n:start_idx_n + n, start_idx_m:start_idx_m + m] = PP_uniform\n",
    "            start_idx_n += n\n",
    "            start_idx_m += m\n",
    "\n",
    "        # Create the complete B matrices with all predictor variables\n",
    "        k = B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1] + B5.shape[1] + B6.shape[1] + B_uniform.shape[1] - 3\n",
    "        B = np.hstack((X, B1, B2, B3, B4, B5, B6, B_uniform))\n",
    "        B_test = np.hstack((B1_test, B2_test, B3_test, B4_test, B5_test, B6_test, B_uniform_test))\n",
    "        B_ext = np.hstack((B1_ext, B2_ext, B3_ext, B4_ext, B5_ext, B6_ext, B_uniform))\n",
    "        \n",
    "        # Create the model\n",
    "        model = gp.Model(\"ConstrainedOptimization\")\n",
    "\n",
    "        # Create decision variables\n",
    "        theta = {j: model.addVar(lb=-GRB.INFINITY, name=f\"theta_{j}\") for j in range(k + 4)}\n",
    "        s = {l: model.addVar(vtype=GRB.BINARY, name=f\"s_{l}\") for l in range(20)}\n",
    "\n",
    "        # Add monotonicity and concavity constraints based on the list of constraints\n",
    "        if 'f2_concavity' in constraints:\n",
    "            for i in range(B2.shape[1] - 2):\n",
    "                model.addConstr(theta[1 + B1.shape[1] + i + 2] - 2 * theta[1 + B1.shape[1] + i + 1] + theta[1 + B1.shape[1] + i] <= 0, name=f\"f2_concavity_{i}\")\n",
    "        if 'f3_monotonicity' in constraints:\n",
    "            for i in range(B3.shape[1] - 1):\n",
    "                model.addConstr(theta[1 + B1.shape[1]+ B2.shape[1] + i + 1] - theta[1 + B1.shape[1] + B2.shape[1] + i] >= 0, name=f\"f3_monotonicity_{i}\")\n",
    "        if 'f4_convexity' in constraints:\n",
    "            for i in range(B4.shape[1] - 2):\n",
    "                model.addConstr(theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + i + 2] - 2 * theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + i + 1] + theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + i] >= 0, name=f\"f4_convexity_{i}\")\n",
    "        if 'x8_monotonicity_decrec' in constraints: \n",
    "            for i in range(B1.shape[1] - 1):\n",
    "                model.addConstr(theta[1 + 7 * B1.shape[1] +i + 1] - theta[1 + 7 * B1.shape[1] + i] <= 0, name=f\"x8_monotonicity_decrec_{i}\")\n",
    "        if 'x15_concavity' in constraints:\n",
    "            for i in range(B1.shape[1] - 2):\n",
    "                model.addConstr(theta[1 + 14 * B1.shape[1] + i + 2] - 2 * theta[1 + 14 * B1.shape[1] + i + 1] + theta[1 + 14 * B1.shape[1] + i] <= 0, name=f\"x15_concavity_{i}\")     \n",
    "        if 'x19_convexity' in constraints:\n",
    "            for i in range(B1.shape[1] - 2):\n",
    "                model.addConstr(theta[1 + 18 * B1.shape[1] + i + 2] - 2 * theta[1 + 18 * B1.shape[1] + i + 1] + theta[1 + 18 * B1.shape[1] + i] >= 0, name=f\"x19_convexity_{i}\")      \n",
    "\n",
    "        # Objective function\n",
    "        obj = gp.quicksum((y_train[i] - gp.quicksum(B[i][j] * theta[j] for j in range(k + 4))) ** 2 for i in range(len(y_train)))\n",
    "        penalization = gp.quicksum(theta[i] * PP[i, j] * theta[j] for j in range(PP.shape[1]) for i in range(PP.shape[0]))\n",
    "        obj += penalization\n",
    "        model.setObjective(obj, GRB.MINIMIZE)\n",
    "\n",
    "        # Variable selection constraint\n",
    "        model.addConstr(gp.quicksum(s[l] for l in range(20)) <= K)\n",
    "\n",
    "        M = 1000\n",
    "\n",
    "        for l in range(20):\n",
    "            for m_l in range(B1.shape[1]):\n",
    "                model.addConstr(-M * s[l] <= theta[1 + B1.shape[1] * l + m_l], name=f\"constraint1_{l}_{m_l}\")\n",
    "                model.addConstr(theta[1 + B1.shape[1] * l + m_l] <= M * s[l], name=f\"constraint2_{l}_{m_l}\")\n",
    "\n",
    "        # Optimize the model\n",
    "        model.optimize()\n",
    "        \n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            opt_theta = [theta[j].X for j in range(k + 4)]\n",
    "            \n",
    "            # Calculate the predictions \n",
    "            y_pred = np.dot(B_test, opt_theta[1:])\n",
    "            \n",
    "            # Compute the MAE and MSE\n",
    "            mae = np.mean(np.abs(y_pred - y_test))\n",
    "            mse = np.mean((y_pred - y_test) ** 2)\n",
    "            \n",
    "            # Calculate f(x) estimates for each data set\n",
    "            f1_est = np.dot(B1_ext, opt_theta[1:1 + B1.shape[1]])\n",
    "            f2_est = np.dot(B2_ext, opt_theta[1 + B1.shape[1]:1 + B1.shape[1] + B2.shape[1]])\n",
    "            f3_est = np.dot(B3_ext, opt_theta[1 + B1.shape[1] + B2.shape[1]:1 + B1.shape[1] + B2.shape[1] + B3.shape[1]])\n",
    "            f4_est = np.dot(B4_ext, opt_theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1]:1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1]])\n",
    "            f5_est = np.dot(B5_ext, opt_theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1]:1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1] + B5.shape[1]])\n",
    "            f6_est = np.dot(B6_ext, opt_theta[1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1] + B5.shape[1]:1 + B1.shape[1] + B2.shape[1] + B3.shape[1] + B4.shape[1] + B5.shape[1] + B6.shape[1]])\n",
    "        \n",
    "            solution = {\n",
    "                \"status\": \"Optimal solution found\", \n",
    "                \"theta\": {j: theta[j].X for j in range(k + 4)}, \n",
    "                \"s\": {l: s[l].X for l in range(20)}, \n",
    "                \"mse\": mse ,\n",
    "                \"mae\": mae, \n",
    "                \"x_train\": [x1_train, x2_train, x3_train, x4_train, x5_train, x6_train],\n",
    "                \"x_train_ext\": [x1_train_ext, x2_train_ext, x3_train_ext, x4_train_ext, x5_train_ext, x6_train_ext],\n",
    "                \"f_train\": [f1_train, f2_train, f3_train, f4_train, f5_train, f6_train],\n",
    "                \"f_train_ext\": [f1_train_ext, f2_train_ext, f3_train_ext, f4_train_ext, f5_train_ext, f6_train_ext],\n",
    "                \"f_est\": [f1_est, f2_est, f3_est, f4_est, f5_est, f6_est]\n",
    "             }\n",
    "\n",
    "        else:\n",
    "            solution = {\"status\": \"No optimal solution found\"}\n",
    "\n",
    "        return solution\n",
    "\n",
    "    solution = optimization_function(x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x_uniform_train, x1_train_ext, x2_train_ext, x3_train_ext, x4_train_ext, x5_train_ext, x6_train_ext, x1_test, x2_test, x3_test, x4_test, x5_test, x6_test, x_uniform_test, y_train, y_test, K, num_nodes, lambdas, constraints)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value of the SNR depends on the value of g, for each Scenario\n",
    "g = 1         #SNR=1\n",
    "#g = 0.5      #SNR=2\n",
    "#g = 0.25     #SNR=4\n",
    "\n",
    "results_1_1 = [] # Create an empty list to store the results\n",
    "# Run a loop for 50 iterations\n",
    "for i in range(50):\n",
    "    print('Simulation:',i+1)\n",
    "    results_1_1.append(run_simulation(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics of the performance of the model:\n",
    "# Initialize empty lists to store metrics for each simulation\n",
    "tpr_list = []  \n",
    "tnr_list = []  \n",
    "acc_list = [] \n",
    "mae_list = []  \n",
    "mse_list = []  \n",
    "\n",
    "\n",
    "for i, result in enumerate(results_1_1):\n",
    "    if result[\"status\"] == \"Optimal solution found\":\n",
    "        # Get the variables selected by the model and error metrics\n",
    "        s_selected = result[\"s\"]  # Selected variables by the model\n",
    "        mae = result[\"mae\"]  # Mean Absolute Error\n",
    "        mse = result[\"mse\"]  # Mean Squared Error\n",
    "        \n",
    "        # Initialize counters for TPR, TNR, and Accuracy\n",
    "        true_positives = 0  \n",
    "        false_negatives = 0  \n",
    "        true_negatives = 0  \n",
    "        false_positives = 0  \n",
    "        \n",
    "        for j in range(6):\n",
    "            if s_selected[j] == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "        \n",
    "        for j in range(6, 20):\n",
    "            if s_selected[j] == 0:\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                \n",
    "        total_positives = true_positives + false_negatives\n",
    "        total_negatives = true_negatives + false_positives\n",
    "        \n",
    "        # Calculate True Positive Rate (TPR) \n",
    "        if total_positives > 0:\n",
    "            tpr = true_positives / 6  # Divide by 6, the total number of relevant variables\n",
    "        else:\n",
    "            tpr = 0.0 \n",
    "        \n",
    "        # Calculate True Negative Rate (TNR) \n",
    "        if total_negatives > 0:\n",
    "            tnr = true_negatives / 14  # Divide by 14, the total number of irrelevant variables\n",
    "        else:\n",
    "            tnr = 0.0 \n",
    "        \n",
    "        # Calculate accuracy as the proportion of correct classifications\n",
    "        accuracy = (true_positives + true_negatives) / 20.0  # 20 variables in total (6 relevant + 14 irrelevant)\n",
    "        \n",
    "        # Append calculated metrics to their respective lists\n",
    "        tpr_list.append(tpr)\n",
    "        tnr_list.append(tnr)\n",
    "        acc_list.append(accuracy)\n",
    "        mae_list.append(mae)\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "    else:\n",
    "        print(f\"No optimal solution found for Simulation {i+1}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the metrics\n",
    "tpr_mean = np.mean(tpr_list)  \n",
    "tpr_std = np.std(tpr_list)  \n",
    "\n",
    "tnr_mean = np.mean(tnr_list)  \n",
    "tnr_std = np.std(tnr_list)  \n",
    "\n",
    "acc_mean = np.mean(acc_list)  \n",
    "acc_std = np.std(acc_list)  \n",
    "\n",
    "mae_mean = np.mean(mae_list)  \n",
    "mae_std = np.std(mae_list) \n",
    "\n",
    "mse_mean = np.mean(mse_list)  \n",
    "mse_std = np.std(mse_list) \n",
    "\n",
    "# Create a DataFrame to summarize the mean and standard deviation of the metrics\n",
    "summary_df_1_1 = pd.DataFrame({\n",
    "    \"Metric\": [\"TPR\", \"TNR\", \"ACC\", \"MAE\", \"MSE\"],\n",
    "    \"Mean\": [tpr_mean, tnr_mean, acc_mean, mae_mean, mse_mean],\n",
    "    \"Standard Deviation\": [tpr_std, tnr_std, acc_std, mae_std, mse_std]\n",
    "})\n",
    "\n",
    "print(\"Summary of Results:\")\n",
    "print(summary_df_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the result of the simulation with the lowest MSE\n",
    "min_mse = float('inf')  \n",
    "best_result_1_1 = None  \n",
    "\n",
    "\n",
    "for result in results_1_1:\n",
    "    # Check if the simulation found an optimal solution and has a lower MSE than the current minimum\n",
    "    if result[\"status\"] == \"Optimal solution found\" and result[\"mse\"] < min_mse:\n",
    "        min_mse = result[\"mse\"]  \n",
    "        best_result_1_1 = result  # Store the result with the lowest MSE\n",
    "\n",
    "\n",
    "if best_result_1_1 is not None:\n",
    "    # Extract variables from the best simulation result\n",
    "    opt_theta = best_result_1_1[\"theta\"]  \n",
    "    x_train_list = best_result_1_1[\"x_train\"]  \n",
    "    x_train_ext_list = best_result_1_1[\"x_train_ext\"]  \n",
    "    f_train_list = best_result_1_1[\"f_train\"]  \n",
    "    f_train_ext_list = best_result_1_1[\"f_train_ext\"] \n",
    "    f_est_list = best_result_1_1[\"f_est\"]  \n",
    "\n",
    "    # Print the MSE of the best simulation\n",
    "    print(best_result_1_1[\"mse\"])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (x_train, x_train_ext, f_train, f_train_ext, f_est, title) in enumerate(zip(\n",
    "            x_train_list, x_train_ext_list, f_train_list, f_train_ext_list, f_est_list,\n",
    "            ['Predicted Curve 1 vs. Simulated Data', 'Predicted Curve 2 vs. Simulated Data',\n",
    "             'Predicted Curve 3 vs. Simulated Data', 'Predicted Curve 4 vs. Simulated Data',\n",
    "             'Predicted Curve 5 vs. Simulated Data', 'Predicted Curve 6 vs. Simulated Data']\n",
    "        )):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        \n",
    "        # Scatter plot of the training data (blue dots)\n",
    "        plt.scatter(x_train, f_train, color='blue', label=f'Data Set {i + 1}')\n",
    "        \n",
    "        # Plot of the theoretical curve (black line)\n",
    "        plt.plot(x_train_ext, f_train_ext, color='black', label=f'Theoretical Curve {i + 1}')\n",
    "        \n",
    "        # Plot of the model's predicted curve (red line)\n",
    "        plt.plot(x_train_ext, f_est, color='red', linestyle='-', label=f'Predictions {i + 1}')\n",
    "        \n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No optimal solution found in the 50 simulations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
